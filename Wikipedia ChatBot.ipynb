{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3b0ac-337f-4fba-a2e7-e59c2745cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìö Enter the topic you want to research on Wikipedia:  Dhoni\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching Wikipedia for: Dhoni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\nares\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Created a chunk of size 2174, which is longer than the specified 500\n",
      "Created a chunk of size 773, which is longer than the specified 500\n",
      "Created a chunk of size 1431, which is longer than the specified 500\n",
      "Created a chunk of size 1154, which is longer than the specified 500\n",
      "Created a chunk of size 1854, which is longer than the specified 500\n",
      "Created a chunk of size 515, which is longer than the specified 500\n",
      "Created a chunk of size 2256, which is longer than the specified 500\n",
      "Created a chunk of size 639, which is longer than the specified 500\n",
      "Created a chunk of size 684, which is longer than the specified 500\n",
      "Created a chunk of size 2114, which is longer than the specified 500\n",
      "Created a chunk of size 1946, which is longer than the specified 500\n",
      "Created a chunk of size 745, which is longer than the specified 500\n",
      "Created a chunk of size 986, which is longer than the specified 500\n",
      "Created a chunk of size 519, which is longer than the specified 500\n",
      "Created a chunk of size 660, which is longer than the specified 500\n",
      "Created a chunk of size 1587, which is longer than the specified 500\n",
      "Created a chunk of size 1469, which is longer than the specified 500\n",
      "Created a chunk of size 1052, which is longer than the specified 500\n",
      "Created a chunk of size 940, which is longer than the specified 500\n",
      "Created a chunk of size 1521, which is longer than the specified 500\n",
      "Created a chunk of size 2218, which is longer than the specified 500\n",
      "Created a chunk of size 1146, which is longer than the specified 500\n",
      "Created a chunk of size 2338, which is longer than the specified 500\n",
      "Created a chunk of size 725, which is longer than the specified 500\n",
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 3118, which is longer than the specified 500\n",
      "Created a chunk of size 772, which is longer than the specified 500\n",
      "Created a chunk of size 1868, which is longer than the specified 500\n",
      "Created a chunk of size 1189, which is longer than the specified 500\n",
      "Created a chunk of size 1033, which is longer than the specified 500\n",
      "Created a chunk of size 704, which is longer than the specified 500\n",
      "Created a chunk of size 920, which is longer than the specified 500\n",
      "Created a chunk of size 615, which is longer than the specified 500\n",
      "Created a chunk of size 747, which is longer than the specified 500\n",
      "Created a chunk of size 2161, which is longer than the specified 500\n",
      "Created a chunk of size 727, which is longer than the specified 500\n",
      "Created a chunk of size 540, which is longer than the specified 500\n",
      "Created a chunk of size 708, which is longer than the specified 500\n",
      "Created a chunk of size 1017, which is longer than the specified 500\n",
      "Created a chunk of size 1707, which is longer than the specified 500\n",
      "Created a chunk of size 1330, which is longer than the specified 500\n",
      "Created a chunk of size 1073, which is longer than the specified 500\n",
      "Created a chunk of size 535, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 24 Wikipedia document(s).\n",
      "‚úÇÔ∏è Splitting documents into smaller chunks...\n",
      "‚úÖ Created 103 text chunks.\n",
      "üß† Embedding documents using HuggingFace embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\AppData\\Local\\Temp\\ipykernel_29536\\1707039991.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ FAISS vector store created.\n",
      "üß† Loading Falcon-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a89038ab81d4d9bacfa497dbdbbdd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chatbot created successfully!\n",
      "\n",
      "üí¨ Ask your questions (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Who is Dhoni ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS  # This import should work now\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Wikipedia page\n",
    "def load_wiki(query):\n",
    "    print(f\"üîç Searching Wikipedia for: {query}\")\n",
    "    loader = WikipediaLoader(query=query, lang=\"en\")\n",
    "    docs = loader.load()\n",
    "    print(f\"üìÑ Loaded {len(docs)} Wikipedia document(s).\")\n",
    "    return docs\n",
    "\n",
    "# Step 2: Split text into chunks\n",
    "def split_docs(docs):\n",
    "    print(\"‚úÇÔ∏è Splitting documents into smaller chunks...\")\n",
    "    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Created {len(chunks)} text chunks.\")\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Embed chunks using HuggingFace embeddings\n",
    "def embed_documents(documents):\n",
    "    print(\"üß† Embedding documents using HuggingFace embeddings...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"üì¶ FAISS vector store created.\")\n",
    "    return vector_store\n",
    "\n",
    "# Step 4: Load Falcon or Mistral open-source LLM\n",
    "def load_llm():\n",
    "    try:\n",
    "        print(\"üß† Loading Falcon-7B-Instruct...\")\n",
    "        model_id = \"tiiuae/falcon-7b-instruct\"  # or try \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0.7)\n",
    "        return HuggingFacePipeline(pipeline=pipe)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Step 5: Create RAG Chatbot\n",
    "def create_chatbot(query):\n",
    "    docs = load_wiki(query)\n",
    "    splitted_docs = split_docs(docs)\n",
    "    vector_store = embed_documents(splitted_docs)\n",
    "    llm = load_llm()\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    print(\"ü§ñ Chatbot created successfully!\")\n",
    "    return qa_chain\n",
    "\n",
    "# Step 6: Interactive Chat\n",
    "def start_chat():\n",
    "    topic = input(\"üìö Enter the topic you want to research on Wikipedia: \")\n",
    "    try:\n",
    "        chatbot = create_chatbot(topic)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create chatbot: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüí¨ Ask your questions (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        question = input(\"You: \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        try:\n",
    "            response = chatbot.invoke({\"query\": question})  # ‚úÖ invoke instead of deprecated run\n",
    "            print(f\"\\nü§ñ Bot: {response['result']}\\n\")\n",
    "\n",
    "            # Optional: Show sources\n",
    "            for i, doc in enumerate(response[\"source_documents\"]):\n",
    "                print(f\"üìÑ Source {i+1}: {doc.metadata.get('title', 'Wikipedia')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e674f73-6206-488b-a4e8-b1ce81dcfaca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
